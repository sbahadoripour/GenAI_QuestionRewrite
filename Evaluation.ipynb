{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8147fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraies\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.evaluation import load_evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Rewrite the following disfluent question into a fluent and understandable question without answering it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infere(data, out_path):\n",
    "    '''\n",
    "    Inference code for dataframe\n",
    "    '''\n",
    "    pred = []  # Initialize a list to store predictions\n",
    "    for index, row in data.iterrows():\n",
    "        print(f\"{index}/{len(data)}\")  # Print the current progress\n",
    "        result = get_completion(query=row['disfluent'], model=model, tokenizer=tokenizer)  # Get the completion result\n",
    "        result = result[result.find('assistant\\n\\n') + 11 :]\n",
    "        pred.append(result[:result.find('?') + 1])  # Extract the relevant part of the result\n",
    "    data['llama'] = pred  # Add the predictions to the DataFrame\n",
    "\n",
    "    data.to_csv(out_path, index=False)  # Save the DataFrame to a CSV file\n",
    "\n",
    "def model_eval(data, out_path):\n",
    "    '''\n",
    "    Model evaluation based on exact match, embedding distance, and string distance.\n",
    "    '''\n",
    "    embedding_model = HuggingFaceEmbeddings()  # Initialize the embedding model\n",
    "    hf_evaluator = load_evaluator(\"embedding_distance\", embeddings=embedding_model)  # Load the embedding distance evaluator\n",
    "    evaluator = load_evaluator(\"exact_match\", ignore_case=True, ignore_numbers=True, ignore_punctuation=True)  # Load the exact match evaluator\n",
    "    sm_evaluator = load_evaluator(\"string_distance\")  # Load the string distance evaluator\n",
    "    \n",
    "    dist = []  # Initialize a list to store embedding distances\n",
    "    exact = []  # Initialize a list to store exact match scores\n",
    "    string_match = []  # Initialize a list to store string match scores\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        dist.append(hf_evaluator.evaluate_strings(prediction=data.iloc[i]['llama_revised'].strip(), reference=data.iloc[i]['original'].strip())['score'])  # Evaluate embedding distance\n",
    "        exact.append(evaluator.evaluate_strings(prediction=data.iloc[i]['llama_revised'].strip(), reference=data.iloc[i]['original'].strip())['score'])  # Evaluate exact match\n",
    "        string_match.append(sm_evaluator.evaluate_strings(prediction=data.iloc[i]['llama_revised'].strip(), reference=data.iloc[i]['original'].strip())['score'])  # Evaluate string match\n",
    "        \n",
    "    data['embed_dist'] = dist  # Add embedding distances to the DataFrame\n",
    "    data['exact_match'] = exact  # Add exact match scores to the DataFrame\n",
    "    data['string_distance'] = string_match  # Add string match scores to the DataFrame\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Exact match: {sum(data['exact_match'].tolist())}\")\n",
    "    print(f\"Embedding distance: {round(sum(data['embed_dist'].tolist()) / len(data), 4)}\")\n",
    "    print(f\"String distance: {round(sum(data['string_distance'].tolist()) / len(data), 4)}\")\n",
    "    \n",
    "    data.to_csv(out_path, index=False)  # Save the DataFrame to a CSV file\n",
    "\n",
    "def get_completion(query, model, tokenizer):\n",
    "    '''\n",
    "    Inferencing function for single instance.\n",
    "    '''\n",
    "    device = \"cuda:0\"  # Specify the device to use for computation\n",
    "    \n",
    "    conversation = []  # Initialize a list to store the conversation\n",
    "    # Define the system and user messages\n",
    "    system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
    "    user_message = {\"role\": \"user\", \"content\": query}\n",
    "\n",
    "    # Insert the system message at the beginning of the conversation\n",
    "    conversation.insert(0, system_message)\n",
    "    # Append the user message to the conversation\n",
    "    conversation.append(user_message)\n",
    "    \n",
    "    # Format the conversation using the tokenizer\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    \n",
    "    # Encode the prompt into tensors\n",
    "    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Move the encoded inputs to the specified device\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=256, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    # Decode the generated response\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded  # Return the decoded response\n",
    "\n",
    "def json_to_df(data):\n",
    "    '''\n",
    "    Convert data from json to dataframe.\n",
    "    '''\n",
    "    # Initialize lists to store 'original' and 'disfluent' values\n",
    "    o = []\n",
    "    d = []\n",
    "    \n",
    "    # Iterate through each key in the data dictionary\n",
    "    for key in data.keys():\n",
    "        # Append the 'original' and 'disfluent' values to their respective lists\n",
    "        o.append(data[key]['original'])\n",
    "        d.append(data[key]['disfluent'])\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame()\n",
    "    df['original'] = o\n",
    "    df['disfluent'] = d\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66defc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json files.\n",
    "with open(\"train.json\" , \"r\") as f:\n",
    "    train_json = json.loads(f.read())\n",
    "\n",
    "with open(\"dev.json\" , \"r\") as f:\n",
    "    dev_json = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert json to dataframe\n",
    "train = json_to_df(train_json)\n",
    "dev = json_to_df(dev_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b831d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"merged_model_llama\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"merged_model_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07fa45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get output for train data\n",
    "infere(train,\"train_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34427cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for train data\n",
    "model_eval(train,\"train_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get output for dev data\n",
    "infere(dev,\"dev_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for dev data\n",
    "model_eval(dev,\"dev_pred.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
